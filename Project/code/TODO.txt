STEPS TODO PEDRO :

- AlTER de config.py to ypur project root but try to maintain the same logic and them add this file to the gitignore

- RUN the config.py file to create the intermediate and raw files as well as those inside of them

- ADD manually raw data into the raw folder from the project/raw path in the teacher's data (dont worry already added to the git ignore so no problems in commit)




TODO:
- Check the python files inside of the b-line detection folder analyse them to check any disruptions that need to be altered, DONE

- Fix the 5_get_processed_frames file, DONE

- Continue the github steps, use only the frame level classfication part, DONE
- Choose more transformer models from hugging face is feasable (it is, just needs more search), DONE

- Alter or add in the training runs a transformer based model, DONE

- AlTER THE EXCEL of the transformer models to fit the new models and its specifications
- Choose only the transformer models from the intermediate/models folder
- Erase the non transformer models from the intermediate/models folder
- Analyse the evaluation folder
- Obtain results 
- Write Paper
- Don't forget to cite the authors of the "Deep Learning for Detection and Localization of B-Lines in Lung Ultrasound" paper


14/05 and 15/05
- Did the steps in the github plus some exploration by myself, we dont need to run the steps 1 to 4 in the data preparation folder as the respective .pkl files are already generated and in the supplementary_files folder
and were after copied to the intermediate/annotation folder and intermediate/info folder

- There's a problem when running the 5_get_processed_frames file so i didnt generate the png images for each frame YET!!

- They have already in the frame level classfication tested some transformers models, maybe we will use those and add more to extend the test and results

- Added to the intermediate/models folder all of the frame level models that they trained


16/05
- Altered the line 21 (frames = 1 ) in the script 2_create_dataset to generate the positive an negative instances labels from the annotations folder
- Created the labelset task = 'classification'  #classification and frames_1 to do the frame level classification 
folder = 'frames_1' chnaging line 27 and 28 of the labelset script 3_create_labelset

- Received the error below when trying to run the labelset script fixed it by going to C:\Users\guilh\anaconda3\Library\bin a searched for libiomp5md.dll file and eliminated it.
OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. 
That is dangerous, since it can degrade performance or cause incorrect results. 
The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. 
As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results.
 For more information, please see http://www.intel.com/software/products/support/.


 18/05 and 19/05

 - Created the official training runs for the scheduled runs wiht just the frame leve lclassification models, and changed the epochs from 100 to 10 

 - resolved bugs
 - tested a single run for the vision trnsformer model 


20/05 and 21/05

- Tested a scheduled run of only vision transformers models with gpu (much faster than with cpu), sould be tyhe 0001_Vision transformer to 0004 so 5 models with 10 epochs each
- Try a evaluation of these 5 models
- 

21/05 e 22/05
ANTES de qualquer treino o file excel tem de ter o finished com falsos por baixo (mudar manuaolmente caso ja tenha sido feito um treino anteriormente) 
start_run	end_run	best_val_loss e essas 3 colunas tem de estar vazias

finished

- All of thius model referredto the timm library , timm/model......
- Add model swin transformer 'swin_tiny_patch4_window7_224.ms_in1k' from hugging face
- Add model DeiT transformer 'deit3_base_patch16_224.fb_in1k' from hugging face
- Add model DeiT transformer 'cait_m36_384.fb_dist_in1k' from hugging face
- Running the same 5 models as before (network training) but with 100 epochs each to check if the results are better
- Fazer evaluation apos o training
